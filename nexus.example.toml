# Nexus Configuration
# Copy this file to nexus.toml and customize

[server]
host = "0.0.0.0"
port = 8000
request_timeout_seconds = 300

[discovery]
# Auto-discover backends via mDNS
enabled = true
# Service types to browse for (trailing dot is optional - added automatically)
service_types = ["_ollama._tcp.local", "_llm._tcp.local"]
# Grace period before removing disappeared backends (seconds)
grace_period_seconds = 60

[health_check]
enabled = true
interval_seconds = 30
timeout_seconds = 5
# Consecutive failures before marking backend unhealthy
failure_threshold = 3
# Consecutive successes before marking backend healthy again
recovery_threshold = 2

[routing]
# Routing strategy: smart | round_robin | priority_only | random
strategy = "smart"
max_retries = 2

[routing.weights]
priority = 50
load = 30
latency = 20

# Model aliases - map common names to your local models
[routing.aliases]
"gpt-4" = "llama3:70b"
"gpt-4-turbo" = "llama3:70b"
"gpt-3.5-turbo" = "mistral:7b"

# Fallback chains - try alternatives if primary unavailable
[routing.fallbacks]
"llama3:70b" = ["qwen2:72b", "mixtral:8x7b"]

# Traffic policies - model-pattern-based routing constraints (optional)
# Policies are evaluated in declaration order (first match wins).
# If no policies are configured, all models are unrestricted (zero-config default).

# Example: Restrict GPT-4 models to local-only backends (no cloud overflow)
# [[routing.policies]]
# model_pattern = "gpt-4*"
# privacy = "restricted"       # restricted | unrestricted
# min_tier = 3                 # Minimum capability tier (1-3, optional)

# Example: Route Claude models to premium backends only
# [[routing.policies]]
# model_pattern = "claude-*"
# privacy = "unrestricted"
# min_tier = 2

# Example: Allow all other models to use any backend
# [[routing.policies]]
# model_pattern = "*"
# privacy = "unrestricted"

# Budget configuration - monthly spending limits for cloud backends (optional)
# If not configured, no budget enforcement is applied (zero-config default).
# [routing.budget]
# monthly_limit_usd = 100.00          # Monthly spending cap in USD
# soft_limit_percent = 75.0           # At 75%: prefer local, reduce cloud scores 50%
# hard_limit_action = "block_cloud"   # At 100%: warn | block_cloud | block_all
# reconciliation_interval_secs = 60   # Background reconciliation interval

# Quality tracking - backend reliability monitoring (Phase 2.5)
# Tracks error rates, TTFT, and success rates to route away from failing backends
[quality]
# Maximum acceptable error rate (1h window) before excluding an agent
# Default: 0.5 (50% errors). Range: 0.0 (exclude any errors) to 1.0 (never exclude)
error_rate_threshold = 0.5

# TTFT threshold in milliseconds - agents above this get penalized in scoring
# Default: 3000ms (3 seconds). Agents with slow time-to-first-token score lower.
ttft_penalty_threshold_ms = 3000

# Interval between quality metric computations in seconds
# Default: 30 seconds. Background loop updates metrics at this interval.
metrics_interval_seconds = 30

# Request queuing - graceful handling of burst traffic (Phase 2.5)
# When all backends are saturated, queue requests instead of immediate 503
[queue]
# Whether request queuing is enabled
# Default: true. When false, saturated requests immediately return 503.
enabled = true

# Maximum number of queued requests
# Default: 100. When 0, queuing is disabled. When full, new requests return 503.
max_size = 100

# Maximum wait time for queued requests in seconds
# Default: 30 seconds. Requests exceeding this timeout return 503 with retry_after.
max_wait_seconds = 30

# Static backend configuration
# Backends can also be auto-discovered via mDNS

[[backends]]
name = "local-ollama"
url = "http://localhost:11434"
type = "ollama"
priority = 1

# [[backends]]
# name = "lmstudio"
# url = "http://localhost:1234"
# type = "lmstudio"
# priority = 2

# [[backends]]
# name = "gpu-server"
# url = "http://192.168.1.100:8000"
# type = "vllm"
# priority = 3

# Cloud backend examples with privacy zones and capability tiers
# All cloud backends require API key via environment variable

# [[backends]]
# name = "openai-cloud"
# url = "https://api.openai.com"
# type = "openai"
# priority = 100
# api_key_env = "OPENAI_API_KEY"
# zone = "open"  # privacy_zone: open | internal | confidential | restricted
# tier = 3       # capability_tier: 1 (fast) | 2 (standard) | 3 (premium)

# [[backends]]
# name = "anthropic-cloud"
# url = "https://api.anthropic.com"
# type = "anthropic"
# priority = 101
# api_key_env = "ANTHROPIC_API_KEY"
# zone = "open"
# tier = 3

# [[backends]]
# name = "google-cloud"
# url = "https://generativelanguage.googleapis.com"
# type = "google"
# priority = 102
# api_key_env = "GOOGLE_API_KEY"
# zone = "open"
# tier = 2

[logging]
# Global log level: trace | debug | info | warn | error
level = "info"

# Log format: pretty (human-readable) | json (for log aggregators)
# Use json for production with ELK, Loki, Splunk, CloudWatch, etc.
format = "pretty"

# Component-specific log levels (optional)
# Useful for debugging specific parts of the system without noise
# [logging.component_levels]
# routing = "debug"    # Detailed routing decisions
# api = "info"         # API request handling
# health = "warn"      # Health check results

# Content logging (opt-in, defaults to false)
# WARNING: When true, request message content will be logged (privacy risk!)
# Only enable for local debugging. Never in production with user data.
# enable_content_logging = false

