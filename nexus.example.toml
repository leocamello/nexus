# Nexus Configuration
# Copy this file to nexus.toml and customize

[server]
host = "0.0.0.0"
port = 8000
request_timeout_seconds = 300

[discovery]
# Auto-discover backends via mDNS
enabled = true
# Service types to browse for (trailing dot is optional - added automatically)
service_types = ["_ollama._tcp.local", "_llm._tcp.local"]
# Grace period before removing disappeared backends (seconds)
grace_period_seconds = 60

[health_check]
enabled = true
interval_seconds = 30
timeout_seconds = 5
# Consecutive failures before marking backend unhealthy
failure_threshold = 3
# Consecutive successes before marking backend healthy again
recovery_threshold = 2

[routing]
# Routing strategy: smart | round_robin | priority_only | random
strategy = "smart"
max_retries = 2

[routing.weights]
priority = 50
load = 30
latency = 20

# Model aliases - map common names to your local models
[routing.aliases]
"gpt-4" = "llama3:70b"
"gpt-4-turbo" = "llama3:70b"
"gpt-3.5-turbo" = "mistral:7b"

# Fallback chains - try alternatives if primary unavailable
[routing.fallbacks]
"llama3:70b" = ["qwen2:72b", "mixtral:8x7b"]

# Static backend configuration
# Backends can also be auto-discovered via mDNS

[[backends]]
name = "local-ollama"
url = "http://localhost:11434"
type = "ollama"
priority = 1
zone = "Restricted"  # Local-only, no cloud overflow

# [[backends]]
# name = "lmstudio"
# url = "http://localhost:1234"
# type = "lmstudio"
# priority = 2

# [[backends]]
# name = "gpu-server"
# url = "http://192.168.1.100:8000"
# type = "vllm"
# priority = 3

# Cloud backend examples (for overflow capacity)
# [[backends]]
# name = "openai-gpt4"
# url = "https://api.openai.com"
# type = "openai"
# priority = 100
# api_key_env = "OPENAI_API_KEY"
# zone = "Open"        # Cloud backend, can receive overflow
# tier = 4             # Capability tier (0-4, auto-detect if omitted)

# [[backends]]
# name = "anthropic-claude"
# url = "https://api.anthropic.com"
# type = "anthropic"
# priority = 110
# api_key_env = "ANTHROPIC_API_KEY"
# zone = "Open"
# tier = 4

[logging]
# Global log level: trace | debug | info | warn | error
level = "info"

# Log format: pretty (human-readable) | json (for log aggregators)
# Use json for production with ELK, Loki, Splunk, CloudWatch, etc.
format = "pretty"

# Component-specific log levels (optional)
# Useful for debugging specific parts of the system without noise
# [logging.component_levels]
# routing = "debug"    # Detailed routing decisions
# api = "info"         # API request handling
# health = "warn"      # Health check results

# Content logging (opt-in, defaults to false)
# WARNING: When true, request message content will be logged (privacy risk!)
# Only enable for local debugging. Never in production with user data.
# enable_content_logging = false

