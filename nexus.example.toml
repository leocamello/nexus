# Nexus Configuration
# Copy this file to nexus.toml and customize

[server]
host = "0.0.0.0"
port = 8000
request_timeout_seconds = 300

[discovery]
# Auto-discover backends via mDNS
enabled = true
# Service types to browse for (trailing dot is optional - added automatically)
service_types = ["_ollama._tcp.local", "_llm._tcp.local"]
# Grace period before removing disappeared backends (seconds)
grace_period_seconds = 60

[health_check]
enabled = true
interval_seconds = 30
timeout_seconds = 5

[routing]
# Routing strategy: smart | round_robin | priority_only | random
strategy = "smart"
max_retries = 2

[routing.weights]
priority = 50
load = 30
latency = 20

# Model aliases - map common names to your local models
[routing.aliases]
"gpt-4" = "llama3:70b"
"gpt-4-turbo" = "llama3:70b"
"gpt-3.5-turbo" = "mistral:7b"

# Fallback chains - try alternatives if primary unavailable
[routing.fallbacks]
"llama3:70b" = ["qwen2:72b", "mixtral:8x7b"]

# Static backend configuration
# Backends can also be auto-discovered via mDNS

[[backends]]
name = "local-ollama"
url = "http://localhost:11434"
type = "ollama"
priority = 1

# [[backends]]
# name = "lmstudio"
# url = "http://localhost:1234"
# type = "lmstudio"
# priority = 2

# [[backends]]
# name = "gpu-server"
# url = "http://192.168.1.100:8000"
# type = "vllm"
# priority = 3

# [[backends]]
# name = "cloud-fallback"
# url = "https://api.openai.com"
# type = "openai"
# priority = 100
# api_key_env = "OPENAI_API_KEY"

[logging]
# Global log level: trace | debug | info | warn | error
level = "info"

# Log format: pretty (human-readable) | json (for log aggregators)
# Use json for production with ELK, Loki, Splunk, CloudWatch, etc.
format = "pretty"

# Component-specific log levels (optional)
# Useful for debugging specific parts of the system without noise
# [logging.component_levels]
# routing = "debug"    # Detailed routing decisions
# api = "info"         # API request handling
# health = "warn"      # Health check results

# Content logging (opt-in, defaults to false)
# WARNING: When true, request message content will be logged (privacy risk!)
# Only enable for local debugging. Never in production with user data.
# enable_content_logging = false

