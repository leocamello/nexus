# Nexus

**One API endpoint. Any backend. Zero configuration.**

Nexus is a distributed LLM model serving orchestrator that unifies heterogeneous inference backends behind a single, intelligent API gateway.

## Features

- ğŸ” **Auto-Discovery**: Automatically finds LLM backends on your network via mDNS
- ğŸ¯ **Intelligent Routing**: Routes requests based on model capabilities and load
- ğŸ”„ **Transparent Failover**: Automatically retries with fallback backends
- ğŸ”Œ **OpenAI-Compatible**: Works with any OpenAI API client
- âš¡ **Zero Config**: Just run it - works out of the box with Ollama

## Supported Backends

| Backend | Status | Notes |
|---------|--------|-------|
| Ollama | âœ… Supported | Auto-discovery via mDNS |
| vLLM | âœ… Supported | Static configuration |
| llama.cpp server | âœ… Supported | Static configuration |
| exo | âœ… Supported | Auto-discovery via mDNS |
| LocalAI | ğŸ”œ Planned | |
| OpenAI (fallback) | ğŸ”œ Planned | Cloud fallback |

## Quick Start

```bash
# Install (from source)
cargo install --path .

# Run with auto-discovery
nexus serve

# Or with a config file
nexus serve --config nexus.toml
```

## Usage

Once running, Nexus exposes an OpenAI-compatible API:

```bash
# List available models
curl http://localhost:8000/v1/models

# Chat completion
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3:70b",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

### With Claude Code / Continue.dev

Point your AI coding assistant to `http://localhost:8000` as the API endpoint.

## Configuration

```toml
# nexus.toml

[server]
host = "0.0.0.0"
port = 8000

[discovery]
enabled = true

[[backends]]
name = "local-ollama"
url = "http://localhost:11434"
type = "ollama"
priority = 1

[[backends]]
name = "gpu-server"
url = "http://192.168.1.100:8000"
type = "vllm"
priority = 2
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Nexus Orchestrator                â”‚
â”‚  - Discovers backends via mDNS              â”‚
â”‚  - Tracks model capabilities                â”‚
â”‚  - Routes to best available backend         â”‚
â”‚  - OpenAI-compatible API                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚           â”‚           â”‚
        â–¼           â–¼           â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Ollama â”‚  â”‚  vLLM  â”‚  â”‚  exo   â”‚
   â”‚  7B    â”‚  â”‚  70B   â”‚  â”‚  32B   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Development

```bash
# Build
cargo build

# Run tests
cargo test

# Run with logging
RUST_LOG=debug cargo run -- serve

# Check formatting
cargo fmt --check

# Lint
cargo clippy
```

## License

Apache License 2.0 - see [LICENSE](LICENSE) for details.

## Related Projects

- [exo](https://github.com/exo-explore/exo) - Distributed AI inference
- [Ollama](https://ollama.ai) - Easy local LLM serving
- [vLLM](https://github.com/vllm-project/vllm) - High-throughput LLM serving
- [LiteLLM](https://github.com/BerriAI/litellm) - Cloud LLM API router
