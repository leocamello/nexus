# Nexus

<!-- Project, Distribution, Rust Ecosystem, & Quality -->
[![Rust](https://img.shields.io/badge/rust-1.87%2B-blue.svg)](https://www.rust-lang.org)
[![License](https://img.shields.io/badge/license-Apache--2.0-blue.svg)](https://github.com/leocamello/nexus/blob/main/LICENSE)
[![GitHub Release](https://img.shields.io/github/v/release/leocamello/nexus)](https://github.com/leocamello/nexus/releases/latest)
[![Docker](https://img.shields.io/docker/v/leocamello/nexus?label=docker&sort=semver)](https://hub.docker.com/r/leocamello/nexus)
[![Crates.io](https://img.shields.io/crates/v/nexus-orchestrator.svg)](https://crates.io/crates/nexus-orchestrator)
[![docs.rs](https://docs.rs/nexus-orchestrator/badge.svg)](https://docs.rs/nexus-orchestrator)
[![codecov](https://codecov.io/gh/leocamello/nexus/branch/main/graph/badge.svg)](https://codecov.io/gh/leocamello/nexus)
[![CI](https://github.com/leocamello/nexus/actions/workflows/ci.yml/badge.svg)](https://github.com/leocamello/nexus/actions/workflows/ci.yml)

**One API endpoint. Any backend. Zero configuration.**

Nexus is a distributed LLM orchestrator that unifies heterogeneous inference backends behind a single, intelligent API gateway. Local first, cloud when needed.

## Features

- ğŸ” **Auto-Discovery** â€” Finds LLM backends on your network via mDNS
- ğŸ¯ **Intelligent Routing** â€” Routes by model capabilities, load, and latency
- ğŸ”„ **Transparent Failover** â€” Retries with fallback backends automatically
- ğŸ”Œ **OpenAI-Compatible** â€” Works with any OpenAI API client
- âš¡ **Zero Config** â€” Just run it â€” works out of the box with Ollama
- ğŸ”’ **Privacy Zones** â€” Structural enforcement prevents data from reaching cloud backends
- ğŸ’° **Budget Management** â€” Token-aware cost tracking with automatic spend limits
- ğŸ“Š **Real-time Dashboard** â€” Monitor backends, models, and requests in your browser
- ğŸ§  **Quality Tracking** â€” Profiles backend response quality to inform routing decisions
- ğŸ“ **Embeddings API** â€” OpenAI-compatible `/v1/embeddings` with capability-aware routing
- ğŸ“‹ **Request Queuing** â€” Holds requests when backends are busy, with priority support

## Supported Backends

| Backend | Status | Discovery |
|---------|--------|-----------|
| [Ollama](https://ollama.ai) | âœ… Supported | mDNS (auto) |
| [LM Studio](https://lmstudio.ai) | âœ… Supported | Static config |
| [vLLM](https://github.com/vllm-project/vllm) | âœ… Supported | Static config |
| [llama.cpp](https://github.com/ggerganov/llama.cpp) | âœ… Supported | Static config |
| [exo](https://github.com/exo-explore/exo) | âœ… Supported | mDNS (auto) |
| [OpenAI](https://openai.com) | âœ… Supported | Static config |

## Quick Start

```bash
# Install from source
cargo install --path .

# Start with auto-discovery (zero config)
nexus serve

# Or with Docker
docker run -d -p 8000:8000 leocamello/nexus
```

Once running, send your first request:

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "llama3:70b", "messages": [{"role": "user", "content": "Hello!"}]}'
```

Point any OpenAI-compatible client to `http://localhost:8000/v1` â€” Claude Code, Continue.dev, OpenAI SDK, or plain curl.

â†’ **[Full setup guide](docs/getting-started.md)** â€” installation, configuration, CLI reference, and more.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Nexus Orchestrator                   â”‚
â”‚  - Discovers backends via mDNS                   â”‚
â”‚  - Tracks model capabilities & quality           â”‚
â”‚  - Routes to best available backend              â”‚
â”‚  - Queues requests when backends are busy        â”‚
â”‚  - OpenAI-compatible API + Embeddings            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚           â”‚           â”‚           â”‚
        â–¼           â–¼           â–¼           â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Ollama â”‚  â”‚  vLLM  â”‚  â”‚  exo   â”‚  â”‚ OpenAI â”‚
   â”‚  7B    â”‚  â”‚  70B   â”‚  â”‚  32B   â”‚  â”‚ cloud  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Documentation

| | Document | What you'll find |
|---|---------|-----------------|
| ğŸš€ | [Getting Started](docs/getting-started.md) | Installation, configuration, CLI, environment variables |
| ğŸ“– | [REST API](docs/api/rest.md) | HTTP endpoints, X-Nexus-* headers, error responses |
| ğŸ”Œ | [WebSocket API](docs/api/websocket.md) | Real-time dashboard protocol |
| ğŸ—ï¸ | [Architecture](docs/architecture.md) | System design, module structure, data flows |
| ğŸ—ºï¸ | [Roadmap](docs/roadmap.md) | Feature index (F01â€“F23), version history, future plans |
| ğŸ”§ | [Troubleshooting](docs/troubleshooting.md) | Common errors, debugging tips |
| â“ | [FAQ](docs/faq.md) | What Nexus is (and isn't), common questions |
| ğŸ¤ | [Contributing](.github/CONTRIBUTING.md) | Dev workflow, coding standards, PR guidelines |
| ğŸ“‹ | [Changelog](CHANGELOG.md) | Release history |
| ğŸ”’ | [Security](.github/SECURITY.md) | Vulnerability reporting |

## License

Apache License 2.0 â€” see [LICENSE](LICENSE) for details.

## Related Projects

- [exo](https://github.com/exo-explore/exo) â€” Distributed AI inference
- [LM Studio](https://lmstudio.ai) â€” Desktop app for local LLMs
- [Ollama](https://ollama.ai) â€” Easy local LLM serving
- [vLLM](https://github.com/vllm-project/vllm) â€” High-throughput LLM serving
- [LiteLLM](https://github.com/BerriAI/litellm) â€” Cloud LLM API router
