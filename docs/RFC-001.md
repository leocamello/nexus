# RFC-001 v2: Nexus Platform Architecture — From Monolithic Router to Controller/Agent Platform

- **Status**: Accepted
- **Authors**: @leocamello
- **Date**: 2026-02-15
- **Revision**: Incorporates v0.4 features (F15–F18), tokenizer strategy, rejection context, loading state, and configuration mapping

---

## Executive Summary

### The Why

Nexus v0.2 is a monolithic stateless router. `Router::select_backend()` is a single function that must answer every routing question. As we add Privacy Zones (F13), Budgets (F14), Quality Tracking (F16), and Request Queuing (F18), this function becomes a god-function encoding every policy simultaneously — creating O(n²) interaction complexity.

Kubernetes solved this with independent controllers reconciling against shared state. We adopt the same internal architecture while preserving our "Zero Config" and "Single Binary" constraints.

### The What

Three-layer architecture:

1. **Nexus Inference Interface (NII)** — A Rust trait abstracting all backends into "Agents" with uniform capabilities, including inference, embeddings, metering, and lifecycle
2. **Control Plane** — A RequestAnalyzer feeds a pipeline of independent Reconcilers (Privacy, Budget, Tier, Quality, Scheduler) that annotate shared state
3. **Embedded Agent Model** — Built-in agents compile into the binary; the NII enables future external plugins

### Constraints Maintained

| Constraint             | Budget                 | Impact                                        |
|------------------------|------------------------|-----------------------------------------------|
| Routing decision       | < 1ms                  | Pipeline adds ~30ns (DashMap lookups)          |
| Total request overhead | < 5ms                  | Unchanged                                     |
| Request analysis       | < 0.5ms                | Heuristic token counting, no model inference   |
| Memory baseline        | < 50MB                 | ~4KB added per agent                           |
| Binary size            | < 20MB                 | Tokenizer behind feature flag                  |
| Config                 | Zero config by default | Advanced policies are optional TOML additions  |

---

## 1. The Nexus Inference Interface (NII)

### Current State

Every backend interaction requires `match backend.backend_type { ... }` branching — health endpoints, model listing, response parsing, and request forwarding all contain type-specific code scattered across `health/parser.rs`, `health/mod.rs`, and `api/completions.rs`. Adding a backend type means touching 3+ files.

### Proposed: The NII Trait

```rust
/// The Nexus Inference Interface.
/// Every backend — built-in or external — implements this trait.
///
/// DESIGN NOTES:
/// - Methods with default implementations return Unsupported — agents opt in.
/// - All async methods must be cancellation-safe. If the client disconnects,
///   the future will be dropped. Agents must clean up resources (e.g., abort
///   in-flight HTTP requests, release semaphore permits).
/// - count_tokens provides a heuristic default (chars/4) so the BudgetReconciler
///   always has data, even for backends without tokenizer support.
#[async_trait]
pub trait InferenceAgent: Send + Sync + 'static {
    /// Unique identifier for this agent instance.
    fn id(&self) -> &str;

    /// Human-readable name.
    fn name(&self) -> &str;

    /// Agent metadata (type, version, capabilities, privacy zone).
    fn profile(&self) -> AgentProfile;

    // ── Discovery ──────────────────────────────────────────────

    /// List models currently available on this agent.
    async fn list_models(&self) -> Result<Vec<ModelCapability>, AgentError>;

    /// Health check. Returns Loading state if a model pull is in progress.
    async fn health_check(&self) -> Result<HealthStatus, AgentError>;

    // ── Inference ──────────────────────────────────────────────

    /// Forward a chat completion request (non-streaming).
    async fn chat_completion(
        &self,
        request: &ChatCompletionRequest,
    ) -> Result<ChatCompletionResponse, AgentError>;

    /// Forward a chat completion request (streaming).
    async fn chat_completion_stream(
        &self,
        request: &ChatCompletionRequest,
    ) -> Result<BoxStream<'_, Result<StreamChunk, AgentError>>, AgentError>;

    /// F17: Generate embeddings.
    async fn embeddings(
        &self,
        _request: &EmbeddingsRequest,
    ) -> Result<EmbeddingsResponse, AgentError> {
        Err(AgentError::Unsupported("embeddings"))
    }

    // ── Lifecycle (v0.5) ───────────────────────────────────────

    /// Load a model onto this agent. Returns when model is ready.
    async fn load_model(&self, _model_id: &str) -> Result<(), AgentError> {
        Err(AgentError::Unsupported("load_model"))
    }

    /// Unload a model from this agent.
    async fn unload_model(&self, _model_id: &str) -> Result<(), AgentError> {
        Err(AgentError::Unsupported("unload_model"))
    }

    // ── Metering ───────────────────────────────────────────────

    /// Count tokens for a given text.
    ///
    /// Tiered strategy:
    /// - Exact: Agent has a loaded tokenizer (tiktoken, sentencepiece)
    /// - Heuristic: Default implementation uses chars/4
    /// - The return value should indicate which tier was used.
    ///
    /// The default heuristic keeps binary size small. Agents with
    /// access to tokenizers (e.g., OpenAIAgent with tiktoken) override
    /// this for audit-grade accuracy.
    async fn count_tokens(
        &self,
        _model_id: &str,
        text: &str,
    ) -> TokenCount {
        TokenCount::Heuristic((text.len() / 4) as u32)
    }

    /// Report resource usage (VRAM, pending requests, etc.)
    async fn resource_usage(&self) -> Result<ResourceUsage, AgentError> {
        Ok(ResourceUsage::default())
    }
}
```

### Supporting Types

```rust
pub struct AgentProfile {
    pub agent_type: String,        // "ollama", "vllm", "openai", "custom"
    pub version: Option<String>,
    pub privacy_zone: PrivacyZone, // From config
    pub capabilities: AgentCapabilities,
}

pub struct AgentCapabilities {
    pub supports_streaming: bool,
    pub supports_embeddings: bool,     // F17
    pub supports_load_unload: bool,
    pub supports_token_counting: bool,
    pub supports_resource_reporting: bool,
}

pub struct ModelCapability {
    pub id: String,
    pub name: String,
    pub context_length: u32,
    pub supports_vision: bool,
    pub supports_tools: bool,
    pub supports_json_mode: bool,
    pub max_output_tokens: Option<u32>,
    pub capability_tier: u8,           // 1-5 (F13)
}

/// Tiered token counting result.
pub enum TokenCount {
    /// Exact count from a loaded tokenizer.
    Exact(u32),
    /// Heuristic estimate (chars/4). Flagged "estimated" in metrics.
    Heuristic(u32),
}

impl TokenCount {
    pub fn value(&self) -> u32 {
        match self {
            TokenCount::Exact(n) | TokenCount::Heuristic(n) => *n,
        }
    }
    pub fn is_exact(&self) -> bool {
        matches!(self, TokenCount::Exact(_))
    }
}

pub struct ResourceUsage {
    pub vram_used_mb: Option<u64>,
    pub vram_total_mb: Option<u64>,
    pub pending_requests: u32,
    pub avg_latency_ms: u32,
    pub loaded_models: Vec<String>,
}

/// Health status with loading state for model lifecycle (F20).
pub enum HealthStatus {
    Healthy,
    Unhealthy,
    /// Agent is pulling/loading a model. Scheduler should queue or fail over.
    Loading { percent: u8, eta_ms: Option<u64> },
    Draining,
}

#[derive(Clone, Copy)]
pub enum PrivacyZone {
    Restricted, // Local-only, never receives cloud overflow
    Open,       // Can receive any traffic
}

pub struct EmbeddingsRequest {
    pub model: String,
    pub input: Vec<String>,
}

pub struct EmbeddingsResponse {
    pub data: Vec<EmbeddingData>,
    pub model: String,
    pub usage: EmbeddingsUsage,
}
```

### Why a Rust Trait, Not gRPC

| Option              | Overhead       | Binary Impact       | Compile Safety |
|---------------------|----------------|---------------------|----------------|
| Rust trait (chosen) | ~1ns vtable    | 0 KB                | Full           |
| gRPC (protobuf)     | ~100µs network | +5 MB (tonic+prost) | Partial        |
| HTTP/REST           | ~500µs network | +0 KB               | None           |

For v0.3–v0.5, NII is an internal Rust trait. If we ever need external plugins (v1.0+), we wrap it in a gRPC adapter on top of the trait — the interface doesn't change.

### Tokenizer Strategy (Binary Size)

The `count_tokens` default uses chars/4 — zero external dependencies. The tiered approach:

| Tier | Method        | Accuracy            | Binary Cost | When Used                         |
|------|---------------|---------------------|-------------|-----------------------------------|
| 1    | chars / 4     | ±30%                | 0 KB        | Default fallback                  |
| 2    | tiktoken-rs   | Exact (GPT models)  | ~1 MB       | OpenAIAgent (built-in)            |
| 3    | HF tokenizers | Exact (open models) | ~8 MB       | Feature flag `audit-grade-metering` |

The conservative multiplier (1.15x) applies to Heuristic counts for budget tracking. Metrics flag heuristic counts as "estimated" so users know the accuracy tier.

---

## 2. The Control Plane Pivot

### Current State: Imperative God-Function

```
Request → extract_requirements() → resolve_alias() → filter_healthy()
        → filter_by_capability() → score_backends() → select_best()
        → proxy_request()
```

Adding privacy, budgets, quality, and queuing to this chain creates a fragile pipeline where ordering matters and every feature interleaves with every other.

### The Request Analyzer (F15)

Before the reconciler pipeline runs, a Request Analyzer constructs the RoutingIntent. This is the "constructor" — it performs fast-path inspection (< 0.5ms) to extract everything reconcilers need.

```rust
/// F15: Speculative Request Analyzer.
/// Extracts requirements from the request payload without model inference.
/// Budget: < 0.5ms.
pub struct RequestAnalyzer;

impl RequestAnalyzer {
    /// Inspect the request and produce a RoutingIntent.
    ///
    /// Detection heuristics:
    /// - Vision: presence of image_url in any message content part
    /// - Tools: presence of `tools` or `functions` array in request
    /// - JSON mode: `response_format.type == "json_object"`
    /// - Token estimate: sum of chars/4 across all messages
    pub fn analyze(
        request: &ChatCompletionRequest,
        config: &RoutingConfig,
    ) -> RoutingIntent {
        let requested_model = request.model.clone();
        let resolved_model = Self::resolve_alias(&requested_model, &config.aliases);

        RoutingIntent {
            request_id: generate_request_id(),
            requested_model,
            resolved_model,
            requirements: RequestRequirements::from_request(request),

            // Initialized empty — reconcilers fill these in
            privacy_constraint: None,
            budget_status: BudgetStatus::Normal,
            min_capability_tier: 0,
            cost_estimate: None,
            candidate_agents: Vec::new(),
            excluded_agents: Vec::new(),
            rejection_reasons: Vec::new(),
        }
    }

    fn resolve_alias(model: &str, aliases: &HashMap<String, String>) -> String {
        // Same 3-level chaining logic as current Router::resolve_alias
    }
}
```

### Internal Resources

These are in-memory Rust structs, not YAML files. Users never see them.

```rust
/// The routing intent — built by RequestAnalyzer, annotated by Reconcilers.
pub struct RoutingIntent {
    pub request_id: String,
    pub requested_model: String,
    pub resolved_model: String,
    pub requirements: RequestRequirements,

    // Annotations — each reconciler writes to its own fields:
    pub privacy_constraint: Option<PrivacyZone>,   // PrivacyReconciler
    pub budget_status: BudgetStatus,                // BudgetReconciler
    pub min_capability_tier: u8,                    // TierReconciler
    pub cost_estimate: Option<CostEstimate>,        // BudgetReconciler
    pub candidate_agents: Vec<String>,              // SchedulerReconciler
    pub excluded_agents: Vec<String>,               // Any reconciler

    /// Tracks why each candidate was dropped.
    /// Used to construct actionable 503 responses ("Honest Always").
    pub rejection_reasons: Vec<RejectionReason>,
}

/// Structured rejection context for actionable error responses.
pub struct RejectionReason {
    pub agent_id: String,
    pub reconciler: String,     // "privacy", "budget", "tier", "scheduler"
    pub reason: String,         // "PrivacyZoneMismatch: agent is Cloud, request requires Restricted"
    pub suggested_action: Option<String>, // "Retry when local backend recovers"
}

/// Per-agent scheduling profile, maintained by background reconciliation loops.
pub struct AgentSchedulingProfile {
    pub agent_id: String,
    pub privacy_zone: PrivacyZone,
    pub capability_tier: u8,
    pub current_load: f64,            // 0.0 - 1.0
    pub latency_ema_ms: u32,
    pub available_models: Vec<String>,
    pub resource_usage: ResourceUsage,
    pub budget_remaining: Option<f64>,

    // F16: Quality Metrics (tracked per rolling 1h window)
    pub error_rate_1h: f32,           // 0.0 - 1.0
    pub avg_ttft_ms: u32,             // Time to first token
    pub last_failure_ts: Option<u64>,
    pub success_rate_24h: f32,        // Longer-term reliability signal
}

pub enum BudgetStatus {
    Normal,
    SoftLimit,   // 80%+ → prefer local agents
    HardLimit,   // 100% → local-only, queue cloud, or reject
}

pub struct CostEstimate {
    pub input_tokens: u32,
    pub estimated_output_tokens: u32,
    pub cost_usd: f64,
    pub token_count_tier: TokenCountTier, // Exact | Heuristic
}

/// The outcome of the reconciler pipeline.
pub enum RoutingDecision {
    /// Route to this agent.
    Route {
        agent_id: String,
        model: String,
        reason: String,
        cost_estimate: Option<CostEstimate>,
    },
    /// F18: All candidates busy or budget-constrained. Hold the request.
    Queue {
        reason: QueueReason,
        estimated_wait_ms: Option<u64>,
        fallback_agent: Option<String>, // Tier-equivalent fallback if wait exceeds timeout
    },
    /// No valid candidates. Return actionable error.
    Reject {
        rejection_reasons: Vec<RejectionReason>,
    },
}

pub enum QueueReason {
    AllAgentsBusy,
    BudgetHardLimit,
    ModelLoading { agent_id: String, progress: u8 },
}
```

### Traffic Policies (User-Configurable)

Advanced users can define routing policies in TOML. Default config has none — zero config philosophy.

```toml
# nexus.toml — Advanced mode (entirely optional)

# Per-model routing constraints
[routing.policies."code-*"]
privacy = "restricted"         # Force local-only for code models
max_cost_per_request = 0.05    # USD cap

[routing.policies."*"]
min_tier = 1                   # Default: accept any tier
fallback_allowed = true

# Per-backend privacy/tier annotations
[[backends]]
name = "local-ollama"
url = "http://localhost:11434"
type = "ollama"
privacy = "restricted"         # NEW: optional, defaults to "open"
tier = 3                       # NEW: optional, defaults to 1

[[backends]]
name = "openai-cloud"
url = "https://api.openai.com"
type = "openai"
api_key_env = "OPENAI_API_KEY"
privacy = "open"
tier = 5
```

```rust
/// Hydrated from [routing.policies.*] at startup.
pub struct TrafficPolicy {
    pub model_pattern: String,          // glob: "code-*", "*"
    pub privacy: Option<PrivacyZone>,
    pub max_cost_per_request: Option<f64>,
    pub min_tier: u8,
    pub fallback_allowed: bool,
}
```

### The Reconciler Pipeline

```rust
#[async_trait]
pub trait Reconciler: Send + Sync {
    fn name(&self) -> &str;

    /// Annotate the routing intent.
    /// May add candidates, exclude agents, or set constraints.
    /// Returns Err only for hard failures (all candidates exhausted).
    async fn reconcile(&self, intent: &mut RoutingIntent) -> Result<(), RoutingError>;
}
```

### Pipeline Flow

```
ChatCompletionRequest
    │
    ▼
┌──────────────────────────────┐
│  RequestAnalyzer (F15)       │  Extract vision/tools/tokens, resolve aliases
│  Budget: < 0.5ms             │  Build RoutingIntent with requirements
└──────────┬───────────────────┘
           ▼
┌──────────────────────────────┐
│  PrivacyReconciler (F13)     │  Set privacy_constraint from TrafficPolicy
│                              │  Exclude cloud agents if restricted
│                              │  Log rejection reasons for excluded agents
└──────────┬───────────────────┘
           ▼
┌──────────────────────────────┐
│  BudgetReconciler (F14)      │  Check spending against limits
│                              │  Set budget_status (Normal/Soft/Hard)
│                              │  Estimate cost via agent.count_tokens()
│                              │  If HardLimit: exclude cloud agents
└──────────┬───────────────────┘
           ▼
┌──────────────────────────────┐
│  TierReconciler (F13)        │  Set min_capability_tier from policy
│                              │  Exclude agents below minimum tier
└──────────┬───────────────────┘
           ▼
┌──────────────────────────────┐
│  QualityReconciler (F16)     │  Exclude agents with error_rate > threshold
│                              │  Penalize agents with high TTFT
│                              │  Feed quality scores to scheduler
└──────────┬───────────────────┘
           ▼
┌──────────────────────────────┐
│  SchedulerReconciler (F06)   │  Score remaining candidates
│                              │  (priority × load × latency × quality)
│                              │  Handle Loading agents → Queue decision
│                              │  Return Route | Queue | Reject
└──────────────────────────────┘
           │
           ▼
    RoutingDecision
      ├── Route { agent, model, reason, cost }
      ├── Queue { reason, wait, fallback }
      └── Reject { rejection_reasons → actionable 503 }
```

### Actionable Error Responses

When the pipeline produces `Reject`, the API layer constructs a 503 with full context:

```json
{
  "error": {
    "message": "No backend available for model 'deepseek-coder:33b'",
    "type": "service_unavailable",
    "code": "no_available_backend",
    "context": {
      "rejection_reasons": [
        {
          "agent": "local-ollama",
          "reason": "Model not loaded",
          "suggested_action": "Load model with: nexus models load deepseek-coder:33b"
        },
        {
          "agent": "openai-cloud",
          "reason": "Privacy zone mismatch (request=restricted, agent=open)",
          "suggested_action": "Remove privacy constraint or use a local backend"
        }
      ],
      "available_models": ["llama3.2:3b", "llama4:latest"],
      "retry_after_seconds": 30
    }
  }
}
```

This makes the "Honest Always" principle programmatic — the `rejection_reasons` accumulated by each reconciler flow directly into the error response.

### Background Reconciliation Loops

Separate from the per-request pipeline, background tasks maintain the AgentSchedulingProfile state:

```rust
// Health + Discovery loop (every health_check.interval_seconds)
async fn health_reconciliation_loop(registry: Arc<AgentRegistry>) {
    loop {
        for agent in registry.agents() {
            let health = agent.health_check().await;
            let models = agent.list_models().await;
            let resources = agent.resource_usage().await;
            registry.update_profile(agent.id(), health, models, resources);
        }
        tokio::time::sleep(interval).await;
    }
}

// Budget tracking loop (every 60s)
async fn budget_reconciliation_loop(tracker: Arc<BudgetTracker>) {
    loop {
        tracker.reconcile_spending().await; // Aggregate costs, check limits
        tokio::time::sleep(Duration::from_secs(60)).await;
    }
}

// Quality metrics loop (every 30s) — F16
async fn quality_reconciliation_loop(registry: Arc<AgentRegistry>) {
    loop {
        for agent_id in registry.agent_ids() {
            // Compute rolling 1h error rate, avg TTFT from request history
            let metrics = registry.compute_quality_metrics(&agent_id);
            registry.update_quality_profile(&agent_id, metrics);
        }
        tokio::time::sleep(Duration::from_secs(30)).await;
    }
}
```

### Request Queuing (F18)

When SchedulerReconciler returns `Queue`, the request enters a bounded in-memory queue:

```rust
pub struct RequestQueue {
    queue: tokio::sync::mpsc::Sender<QueuedRequest>,
    max_size: usize,
    max_wait: Duration,
}

pub struct QueuedRequest {
    pub intent: RoutingIntent,
    pub request: ChatCompletionRequest,
    pub response_tx: oneshot::Sender<Result<ChatCompletionResponse, ApiError>>,
    pub enqueued_at: Instant,
}
```

A background task drains the queue, re-running the reconciler pipeline when agents become available or models finish loading. If `max_wait` expires, the request gets a 503 with `retry_after`.

---

## 3. The Embedded Agent Model

### Agent Factory

```rust
/// Create an agent from config. Users configure backends in TOML;
/// Nexus instantiates the correct agent type automatically.
pub fn create_agent(config: &BackendConfig) -> Arc<dyn InferenceAgent> {
    match config.backend_type {
        BackendType::Ollama   => Arc::new(OllamaAgent::new(config)),
        BackendType::VLLM     => Arc::new(VLLMAgent::new(config)),
        BackendType::OpenAI   => Arc::new(OpenAIAgent::new(config)),
        BackendType::LMStudio => Arc::new(LMStudioAgent::new(config)),
        BackendType::LlamaCpp => Arc::new(GenericOpenAIAgent::new(config)),
        BackendType::Exo      => Arc::new(GenericOpenAIAgent::new(config)),
        _                     => Arc::new(GenericOpenAIAgent::new(config)),
    }
}
```

### Agent Capability Matrix

| Agent              | chat_completion | streaming | embeddings | count_tokens     | load_model     | resource_usage |
|--------------------|-----------------|-----------|------------|------------------|----------------|----------------|
| OllamaAgent        | ✅              | ✅        | ✅         | Heuristic        | ✅ (/api/pull) | ✅ (/api/ps)   |
| OpenAIAgent        | ✅              | ✅        | ✅         | Exact (tiktoken) | ❌             | ❌             |
| LMStudioAgent      | ✅              | ✅        | ✅         | Heuristic        | ❌             | ❌             |
| VLLMAgent          | ✅              | ✅        | ✅         | Heuristic        | ✅             | ✅ (/metrics)  |
| GenericOpenAIAgent | ✅              | ✅        | ❌         | Heuristic        | ❌             | ❌             |

The user's TOML stays the same. Nexus internally creates the right agent. Zero config.

---

## 4. Architecture Topology

```
┌─────────────────────────────────────────────────────────────────────┐
│                      NEXUS BINARY (single process)                  │
│                                                                     │
│  ┌──────────────── CONTROL PLANE ─────────────────────────────────┐ │
│  │                                                                │ │
│  │  ┌──────────────────┐                                          │ │
│  │  │ RequestAnalyzer   │  F15: < 0.5ms payload inspection        │ │
│  │  │ (vision/tools/    │  Builds RoutingIntent                   │ │
│  │  │  tokens/alias)    │                                         │ │
│  │  └────────┬──────────┘                                         │ │
│  │           ▼                                                    │ │
│  │  ┌────────────────────────────────────────────────────────┐    │ │
│  │  │            Reconciler Pipeline (< 1ms total)           │    │ │
│  │  │                                                        │    │ │
│  │  │  Privacy → Budget → Tier → Quality → Scheduler         │    │ │
│  │  │  (F13)    (F14)   (F13)   (F16)     (F06)              │    │ │
│  │  └────────────────────────────┬───────────────────────────┘    │ │
│  │                               │                                │ │
│  │                    ┌──────────┴──────────┐                     │ │
│  │                    │  RoutingDecision     │                     │ │
│  │                    │  Route│Queue│Reject  │                     │ │
│  │                    └─────────────────────┘                     │ │
│  │                                                                │ │
│  │  ┌──────────────────────────────────────────────────────────┐  │ │
│  │  │              Agent Registry (DashMap)                    │  │ │
│  │  │    AgentSchedulingProfiles + ModelIndex + Quality        │  │ │
│  │  └──────────────────────────────────────────────────────────┘  │ │
│  │                                                                │ │
│  │  ┌────────────┐ ┌───────────┐ ┌────────────┐ ┌─────────────┐  │ │
│  │  │ Health     │ │ Budget    │ │ Quality    │ │ mDNS        │  │ │
│  │  │ Loop       │ │ Loop      │ │ Loop (F16) │ │ Discovery   │  │ │
│  │  └────────────┘ └───────────┘ └────────────┘ └─────────────┘  │ │
│  │                                                                │ │
│  │  ┌──────────────────┐                                          │ │
│  │  │ Request Queue    │  F18: Bounded queue with timeout         │ │
│  │  │ (mpsc + drain)   │  Re-runs pipeline when agents free       │ │
│  │  └──────────────────┘                                          │ │
│  └────────────────────────────────────────────────────────────────┘ │
│                                                                     │
│  ┌──────────────── DATA PLANE ────────────────────────────────────┐ │
│  │                                                                │ │
│  │  ┌──────────────┐ ┌──────────────┐ ┌───────────────────────┐  │ │
│  │  │ OllamaAgent  │ │ OpenAIAgent  │ │ LMStudio/Generic/...  │  │ │
│  │  │ (NII impl)   │ │ (NII impl)   │ │ (NII impl)            │  │ │
│  │  │              │ │              │ │                        │  │ │
│  │  │ • inference  │ │ • inference  │ │ • inference            │  │ │
│  │  │ • embeddings │ │ • embeddings │ │ • embeddings           │  │ │
│  │  │ • lifecycle  │ │ • tiktoken   │ │ • heuristic tokens     │  │ │
│  │  │ • /api/ps    │ │              │ │                        │  │ │
│  │  └──────┬───────┘ └──────┬──────┘ └───────────┬────────────┘  │ │
│  └─────────┼────────────────┼────────────────────┼────────────────┘ │
│            │                │                    │                  │
└────────────┼────────────────┼────────────────────┼──────────────────┘
             ▼                ▼                    ▼
         Ollama:11434    api.openai.com      LM Studio:1234
```

---

## 5. Migration Roadmap

### Guiding Principle

No big rewrites. Each phase delivers working software. The existing tests continue to pass throughout. The external API never changes.

### Phase 1: NII Extraction (targets v0.3-alpha)

**Goal**: Define the `InferenceAgent` trait and extract built-in agents from existing code. Ship OpenAIAgent for cloud backends (F12).

New module structure:

```
src/
├── agent/                    # NEW
│   ├── mod.rs               # InferenceAgent trait, AgentProfile, AgentError, types
│   ├── ollama.rs            # OllamaAgent (from health/parser.rs + api/completions.rs)
│   ├── openai.rs            # OpenAIAgent (F12: cloud backends)
│   ├── lmstudio.rs          # LMStudioAgent
│   └── generic.rs           # GenericOpenAIAgent (vLLM, exo, llama.cpp)
```

Steps:

1. Define `InferenceAgent` trait with all methods (including embeddings, count_tokens with defaults)
2. Implement `OllamaAgent` — move health parsing from `health/parser.rs`, request forwarding from `api/completions.rs::proxy_request`
3. Implement `LMStudioAgent`, `GenericOpenAIAgent` — extract type-specific logic
4. Implement `OpenAIAgent` — new, for cloud backends (F12)
5. Registry stores `Arc<dyn InferenceAgent>` alongside existing `Backend` struct (dual storage during migration)
6. Health checker calls `agent.health_check()` + `agent.list_models()` instead of type-specific branching
7. `proxy_request()` calls `agent.chat_completion()` instead of building HTTP requests manually

**What doesn't change**: Router logic, Dashboard, Metrics, Config format, CLI, all existing tests.

### Phase 2: Control Plane (targets v0.3)

**Goal**: Replace `Router::select_backend()` with the reconciler pipeline. Ship Privacy Zones (F13) and Budget Management (F14).

New module structure:

```
src/
├── control/                  # NEW
│   ├── mod.rs               # Reconciler trait, RoutingIntent, pipeline runner
│   ├── analyzer.rs          # RequestAnalyzer (F15 foundation)
│   ├── alias.rs             # AliasReconciler (from routing/mod.rs)
│   ├── privacy.rs           # PrivacyReconciler (F13)
│   ├── budget.rs            # BudgetReconciler (F14)
│   ├── tier.rs              # TierReconciler (F13)
│   └── scheduler.rs         # SchedulerReconciler (from routing scoring)
├── routing/                  # SIMPLIFIED: thin wrapper
│   └── mod.rs               # Router::select_backend() → builds intent → runs pipeline
```

Steps:

1. Define `Reconciler` trait and `RoutingIntent` (with `rejection_reasons`)
2. Extract alias resolution → `AliasReconciler`
3. Extract scoring → `SchedulerReconciler` (with `HealthStatus::Loading` handling)
4. `Router::select_backend()` becomes a thin wrapper: create intent, run pipeline, return result
5. Add `PrivacyReconciler` (F13) — read zone from agent profile, exclude mismatched, log reasons
6. Add `BudgetReconciler` (F14) — track spending, estimate costs, enforce limits
7. Add `TierReconciler` (F13) — enforce minimum capability tier
8. Add `AgentSchedulingProfile` with quality fields (error_rate, TTFT) — populated with defaults initially

**Backward compatibility**: `Router::select_backend()` signature unchanged. All tests pass without modification.

### Phase 2.5: Quality + Queuing (targets v0.4)

**Goal**: Ship Quality Tracking (F16), Embeddings API (F17), Request Queuing (F18), Speculative Router (F15).

Steps:

1. `QualityReconciler` — reads `error_rate_1h`, `avg_ttft_ms` from profile, penalizes poor agents
2. Quality background loop — computes rolling metrics from request history
3. `RequestQueue` — bounded mpsc channel, drain task re-runs pipeline
4. `SchedulerReconciler` returns `Queue` when all candidates are Loading or busy
5. Embeddings endpoint (`POST /v1/embeddings`) — delegates to `agent.embeddings()`
6. `RequestAnalyzer` enhanced with better token estimation heuristics

### Phase 3: Fleet Intelligence (targets v0.5)

**Goal**: Ship Model Lifecycle (F20) and Pre-warming (F19).

Steps:

1. `OllamaAgent.load_model()` → `POST /api/pull`, `resource_usage()` → `GET /api/ps`
2. `FleetReconciler` — analyze request patterns, recommend pre-warming
3. `LifecycleReconciler` — coordinate load/unload, handle `Loading` health state
4. API endpoints: `POST /v1/models/load`, `DELETE /v1/models/{id}`

---

## 6. Performance Analysis

### Hot Path (Per-Request)

| Step                  | Current                      | Proposed                       | Delta |
|-----------------------|------------------------------|--------------------------------|-------|
| RequestAnalyzer (F15) | from_request() ~50ns         | Same logic, new location ~50ns | +0ns  |
| Alias resolution      | HashMap lookup ~10ns         | AliasReconciler ~10ns          | +0ns  |
| Privacy filter        | N/A                          | DashMap lookup ~20ns           | +20ns |
| Budget check          | N/A                          | AtomicU64 read ~5ns            | +5ns  |
| Tier filter           | N/A                          | u8 comparison ~5ns             | +5ns  |
| Quality filter (F16)  | N/A                          | f32 comparison ~5ns            | +5ns  |
| Backend scoring       | Vec scan + arithmetic ~100ns | SchedulerReconciler ~100ns     | +0ns  |
| Queue check (F18)     | N/A                          | Channel capacity check ~5ns    | +5ns  |
| **Total**             | **~160ns**                   | **~200ns**                     | **+40ns** |

**Verdict**: 200ns — well within the 1ms budget with 5000x headroom.

### Memory

| Component             | Size                | Notes                             |
|-----------------------|---------------------|-----------------------------------|
| Per-agent overhead    | ~3KB                | Agent struct + scheduling profile |
| Reconcilers (×6)      | ~3KB                | Lightweight state references      |
| RoutingIntent         | ~256B               | Stack-allocated, per-request      |
| Budget tracker        | ~1KB                | Single instance                   |
| Request queue (F18)   | ~10KB               | Bounded, configurable             |
| Quality metrics (F16) | ~500B/agent         | Rolling window counters           |
| **Total added**       | **~18KB + 3.5KB/agent** | Well under 50MB budget        |

---

## 7. What We Explicitly Do NOT Do

1. **No etcd/persistent store** — All state is in-memory. Restart = fresh from config + discovery.
2. **No YAML/CRD files** — Resources are Rust structs. Users configure via TOML.
3. **No sidecar processes** — All agents are in-process `Arc<dyn InferenceAgent>`.
4. **No gRPC dependency** — NII is a Rust trait. gRPC adapter deferred to v1.0+ if needed.
5. **No breaking API changes** — OpenAI-compatible API untouched. `X-Nexus-*` headers only.
6. **No inference-to-route** — Request analysis is heuristic (chars/4, field presence), never model inference.

---

## 8. Decision Record

| Decision                                 | Choice            | Rationale                                                                       |
|------------------------------------------|-------------------|---------------------------------------------------------------------------------|
| NII as Rust trait vs gRPC                | Rust trait        | Zero overhead, single binary, compile-time safety                               |
| count_tokens default = heuristic         | chars/4           | Keeps binary small; exact counting opt-in per agent                             |
| HealthStatus::Loading state              | Yes               | Prevents routing to agents mid-model-pull (F20 race condition)                  |
| rejection_reasons in RoutingIntent       | Yes               | Enables actionable 503s (Honest Always principle)                               |
| RoutingDecision::Queue variant           | Yes               | F18 needs a valid non-error state; dropping requests is unacceptable            |
| TrafficPolicy in TOML                    | Optional sections | Zero config by default; advanced users add [routing.policies]                   |
| Embeddings in NII Phase 1               | Yes               | Avoids breaking trait signature when F17 ships in v0.4                          |
| Quality fields in Phase 2 profiles       | Default values    | Data structures ready; populated with real data in Phase 2.5                    |
| Phase 1 first (NII before Control Plane) | Yes               | Trait abstraction enables everything; without it, reconcilers still type-branch |

---

## Open Questions

1. **External plugin protocol (v1.0+)**: If out-of-process agents are needed, should NII-over-network use REST (simpler, OpenAI-compatible) or gRPC (faster streaming)? Decision deferred.
2. **Queue sizing**: Should the request queue be global or per-model? Per-model prevents one hot model from starving others but adds complexity.
3. **Quality threshold configuration**: Should the error rate threshold that triggers agent exclusion be configurable per-model, per-agent, or global? Likely global with per-agent override.

---

*This RFC is a discussion document. Implementation begins only after review and approval.*
